{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20201111_NC_MNIST_CA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOA9+CxQBl5QAIJIUMfnaHN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nunocalaim/CA_MNIST/blob/main/20201111_NC_MNIST_CA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDj-Sg9wHwa9"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "\n",
        "import json"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22o5O_2eKVkR"
      },
      "source": [
        "CHANNEL_N = 19 # Number of CA state channels. It includes the 10 possible outputs but not the greyscale channel\n",
        "PER_FILTERS = 80 # Number of filters to perception\n",
        "CLASS_FILTERS = 80 # Number of filters of classifier (can this be diff than PER_FILTERS?)\n",
        "\n",
        "TR_EVOLVE = 20 # Number of time steps to let CA evolve for each input\n",
        "BATCH_SIZE = 16 # number of images per batch\n",
        "BATCH_SIZE = 1 # number of images per batch\n",
        "\n",
        "\n",
        "TR_NO_ITERATIONS = 100000 # number of iterations for the training loop\n",
        "TR_NO_ITERATIONS = 1 # number of iterations for the training loop\n",
        "\n",
        "ADD_NOISE = True # if True then the normal update of the CA has noise added\n",
        "LIVING_THRESHOLD = 0.1 # the grayscale normalised minimum value for which to consider a CA alive\n",
        "FR_THRESHOLD = 0.5 # update only this valyue % of ... actually I don't understad this"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSWIJ8q8KWS_",
        "outputId": "a2fdccde-5e91-4bee-c285-e13ecd4c8ca5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class CAModel(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, channel_n=CHANNEL_N, add_noise=ADD_NOISE):\n",
        "        super().__init__()\n",
        "        self.channel_n = channel_n\n",
        "        self.add_noise = add_noise\n",
        "\n",
        "        self.perceive = tf.keras.Sequential([\n",
        "            Conv2D(PER_FILTERS, 3, activation=tf.nn.relu, padding=\"SAME\"),\n",
        "        ]) # I still don't fully understand what this does. I guess this takes state x of CA and augments it through a conv layer\n",
        "\n",
        "        self.dmodel = tf.keras.Sequential([\n",
        "            Conv2D(CLASS_FILTERS, 1, activation=tf.nn.relu),\n",
        "            Conv2D(self.channel_n, 1, activation=None, kernel_initializer=tf.zeros_initializer),\n",
        "        ])\n",
        "        \n",
        "        self(tf.zeros([1, 3, 3, channel_n + 1])) # dummy call to build the model\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        '''\n",
        "        this function updates the CA for one cycle\n",
        "        x is the current CA state. its shape is (batch_size, height, width, no_channels). \n",
        "            batch_size is BATCH_SIZE.\n",
        "            height and width are 28.\n",
        "            no_channels is 1 + CHANNEL_N, \n",
        "                where the first is the gray image, \n",
        "                the last 10 are the classification predictions,\n",
        "                and the others are there just for fun :)\n",
        "        '''\n",
        "        ds = self.dmodel(self.perceive(x)) # ds will be the state update (of course, we don't want to update the gray image as that is our true input)\n",
        "        gray, state = tf.split(x, [1, self.channel_n], -1)\n",
        "        if self.add_noise:\n",
        "            residual_noise = tf.random.normal(tf.shape(ds), mean=0., stddev=0.02)\n",
        "            ds += residual_noise\n",
        "\n",
        "        update_mask = tf.random.uniform(tf.shape(x[:, :, :, :1])) <= FR_THRESHOLD # I guess this is size (batch, h, w, 1)\n",
        "        living_mask = gray > LIVING_THRESHOLD # I guess this is size (batch, h, w, 1)\n",
        "        residual_mask = update_mask & living_mask # I guess this is size (batch, h, w, 1)\n",
        "        ds *= tf.cast(residual_mask, tf.float32) \n",
        "        state += ds\n",
        "\n",
        "        return tf.concat([gray, state], -1)\n",
        "\n",
        "    @tf.function\n",
        "    def initialize(self, images):\n",
        "        '''\n",
        "        input: images of size (batch, h, w)\n",
        "        output: initial CA state full of 0's for positions other than the images. shape (batch, h, w, 1 + channel_n)\n",
        "        '''\n",
        "        state = tf.zeros([tf.shape(images)[0], 28, 28, self.channel_n]) # size (batch, h, w, channel_n) full of zeros\n",
        "        images = tf.reshape(images, [-1, 28, 28, 1]) # our images we add an extra dimension\n",
        "        return tf.concat([images, state], -1) # just concatenating\n",
        "\n",
        "    @tf.function\n",
        "    def classify(self, x):\n",
        "        '''\n",
        "        The last 10 layers are the classification predictions, one channel\n",
        "        per class. Keep in mind there is no \"background\" class,\n",
        "        and that any loss doesn't propagate to \"dead\" pixels.\n",
        "        '''\n",
        "        return x[:, :, :, -10:]\n",
        "\n",
        "CAModel().perceive.summary()\n",
        "CAModel().dmodel.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function CAModel.call at 0x7eff73fd9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (1, 3, 3, 80)             14480     \n",
            "=================================================================\n",
            "Total params: 14,480\n",
            "Trainable params: 14,480\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function CAModel.call at 0x7eff7401c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (1, 3, 3, 80)             6480      \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (1, 3, 3, 19)             1539      \n",
            "=================================================================\n",
            "Total params: 8,019\n",
            "Trainable params: 8,019\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVpERsj_KYBn"
      },
      "source": [
        "# Training utilities\n",
        "\n",
        "def individual_l2_loss(x, y):\n",
        "    '''\n",
        "    x is the current CA state vector. its shape is (batch_size, height, width, no_channels).\n",
        "    y is the correct label out of 10 possibilities. its shape is (batch_size, height, width, 10) (one-hot)\n",
        "    '''\n",
        "    t = y - ca.classify(x) # basically we want 1's for the correct and 0s for the incorrect digit. its shape is (batch_size, height, width, 10) (one-hot)\n",
        "    return tf.reduce_sum(t ** 2, [1, 2, 3]) / 2 # size batch_size\n",
        "\n",
        "def batch_l2_loss(x, y):\n",
        "    '''\n",
        "    x is the current CA state vector. its shape is (batch_size, height, width, no_channels).\n",
        "    y is the correct label out of 10 possibilities. its shape is (batch_size, height, width, 10) (one-hot)\n",
        "    returns the mean of the loss function\n",
        "    '''\n",
        "    return tf.reduce_mean(individual_l2_loss(x, y))\n",
        "\n",
        "lr = 1e-3 # initial learning rate\n",
        "lr_sched = tf.keras.optimizers.schedules.PiecewiseConstantDecay([int(TR_NO_ITERATIONS*0.3), int(TR_NO_ITERATIONS*0.7)], [lr, lr*0.1, lr*0.01])\n",
        "trainer = tf.keras.optimizers.Adam(lr_sched) # use ADAM optimizer with learning rate schedule\n",
        "\n",
        "loss_log = [] # for plotting of loss function across time\n",
        "\n",
        "def export_model(ca, base_fn):\n",
        "    '''\n",
        "    Saves the models parameters in file name base_fn\n",
        "    '''\n",
        "    ca.save_weights(base_fn)\n",
        "\n",
        "    cf = ca.call.get_concrete_function(x = tf.TensorSpec([None, None, None, CHANNEL_N+1]))\n",
        "    cf = convert_to_constants.convert_variables_to_constants_v2(cf)\n",
        "    graph_def = cf.graph.as_graph_def()\n",
        "    graph_json = MessageToDict(graph_def)\n",
        "    graph_json['versions'] = dict(producer='1.14', minConsumer='1.14')\n",
        "    model_json = {\n",
        "        'format': 'graph-model',\n",
        "        'modelTopology': graph_json,\n",
        "        'weightsManifest': [],\n",
        "    }\n",
        "    with open(base_fn+'.json', 'w') as f:\n",
        "        json.dump(model_json, f)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69UGJ5JrSvQh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHh0U522Lb_7"
      },
      "source": [
        "# Prepare the dataset\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = np.array(x_train / 255.0,).astype(np.float32)\n",
        "x_test = np.array(x_test / 255.0,).astype(np.float32)\n",
        "\n",
        "\n",
        "def to_ten_dim_label(x, y):\n",
        "    '''\n",
        "    input x shape is (no_images, height, width)\n",
        "    input y shape is (no_images,)\n",
        "    output: y_res (no_images, height, width, 10) y_res[b, h, w, i] = 1 if the image b is digit i, and only at the positions h, w where it is alive\n",
        "    '''\n",
        "    y_res = np.zeros(list(x.shape) + [10])\n",
        "    y_expanded = np.broadcast_to(y, x.T.shape).T # broadcast y to match x shape:\n",
        "    y_res[x >= LIVING_THRESHOLD, y_expanded[x >= LIVING_THRESHOLD]] = 1.0 #\n",
        "    return y_res.astype(np.float32)\n",
        "\n",
        "y_train_pic = to_ten_dim_label(x_train, y_train)\n",
        "# y_test_pic = to_ten_dim_label(x_test, y_test)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mXAsQEGRYHe",
        "outputId": "44a1f20f-440a-42a2-bf5a-40e0144b22c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a = np.random.randn(3, 4, 5)\n",
        "b = np.random.randn(3)\n",
        "print(a, b)\n",
        "print(a, np.broadcast_to(b, a.T.shape).T)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 0.93922432  0.89638196 -1.68495572  0.1777811  -0.95294535]\n",
            "  [ 0.64271998 -0.28456121 -0.84549924 -0.32952654  1.03997017]\n",
            "  [ 0.73714633 -1.16163243 -1.27361551  0.66540782  0.09019051]\n",
            "  [-0.47673642  0.97038622  0.06720716  0.89934849 -0.0528374 ]]\n",
            "\n",
            " [[ 0.57851741 -1.00147287 -0.92709125  0.52975749 -0.62648174]\n",
            "  [ 1.38711651  1.11725873  1.7534531  -0.52021237 -0.54599451]\n",
            "  [ 0.27252445  0.47543917 -0.61240874 -1.43600691 -0.73406249]\n",
            "  [-0.3965438   1.0017944  -0.13928547  1.20781554 -0.41215007]]\n",
            "\n",
            " [[-0.00810756  0.23120565 -0.94430096  0.61229871  1.31281622]\n",
            "  [ 1.16026577  1.19121688 -0.35750661 -0.11492323  0.16120102]\n",
            "  [-0.22658267  0.51921208 -0.57271143 -1.18678104 -0.861365  ]\n",
            "  [-1.53595642  2.07688166  0.23652964  0.87284657 -0.12296488]]] [1.75044833 1.26315116 0.14007051]\n",
            "[[[ 0.93922432  0.89638196 -1.68495572  0.1777811  -0.95294535]\n",
            "  [ 0.64271998 -0.28456121 -0.84549924 -0.32952654  1.03997017]\n",
            "  [ 0.73714633 -1.16163243 -1.27361551  0.66540782  0.09019051]\n",
            "  [-0.47673642  0.97038622  0.06720716  0.89934849 -0.0528374 ]]\n",
            "\n",
            " [[ 0.57851741 -1.00147287 -0.92709125  0.52975749 -0.62648174]\n",
            "  [ 1.38711651  1.11725873  1.7534531  -0.52021237 -0.54599451]\n",
            "  [ 0.27252445  0.47543917 -0.61240874 -1.43600691 -0.73406249]\n",
            "  [-0.3965438   1.0017944  -0.13928547  1.20781554 -0.41215007]]\n",
            "\n",
            " [[-0.00810756  0.23120565 -0.94430096  0.61229871  1.31281622]\n",
            "  [ 1.16026577  1.19121688 -0.35750661 -0.11492323  0.16120102]\n",
            "  [-0.22658267  0.51921208 -0.57271143 -1.18678104 -0.861365  ]\n",
            "  [-1.53595642  2.07688166  0.23652964  0.87284657 -0.12296488]]] [[[1.75044833 1.75044833 1.75044833 1.75044833 1.75044833]\n",
            "  [1.75044833 1.75044833 1.75044833 1.75044833 1.75044833]\n",
            "  [1.75044833 1.75044833 1.75044833 1.75044833 1.75044833]\n",
            "  [1.75044833 1.75044833 1.75044833 1.75044833 1.75044833]]\n",
            "\n",
            " [[1.26315116 1.26315116 1.26315116 1.26315116 1.26315116]\n",
            "  [1.26315116 1.26315116 1.26315116 1.26315116 1.26315116]\n",
            "  [1.26315116 1.26315116 1.26315116 1.26315116 1.26315116]\n",
            "  [1.26315116 1.26315116 1.26315116 1.26315116 1.26315116]]\n",
            "\n",
            " [[0.14007051 0.14007051 0.14007051 0.14007051 0.14007051]\n",
            "  [0.14007051 0.14007051 0.14007051 0.14007051 0.14007051]\n",
            "  [0.14007051 0.14007051 0.14007051 0.14007051 0.14007051]\n",
            "  [0.14007051 0.14007051 0.14007051 0.14007051 0.14007051]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KRpKcIhKaBF",
        "outputId": "f33366db-4277-4641-e59a-3277d7211334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        }
      },
      "source": [
        "# Training happens here\n",
        "ca = CAModel()\n",
        "fig, ax = plt.subplots(1, 1);\n",
        "ax.set_title('Loss history (log10)')\n",
        "\n",
        "# Training Step\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    '''\n",
        "    x is the current CA state. its shape is (batch_size, height, width, no_channels).\n",
        "    y is the correct label out of 10 possibilities. its shape is (batch_size, ?)\n",
        "    '''\n",
        "    iter_n = TR_EVOLVE # Number of iterations of the CA for each training step\n",
        "    with tf.GradientTape() as g: # GradientTape does automatic differentiation on the learnable_parameters of our model\n",
        "        for i in tf.range(iter_n): # Basically let time evolve\n",
        "            x = ca(x) # update the CA according to call method? ca(x) = ca.call(x)?\n",
        "        loss = batch_l2_loss(x, y) # compute the scalar loss\n",
        "    grads = g.gradient(loss, ca.weights) # Gradient Tape and Keras doing its magic\n",
        "    grads = [g/(tf.norm(g)+1e-8) for g in grads] # Normalising the gradients uh?\n",
        "    trainer.apply_gradients(zip(grads, ca.weights)) # Keras and ADAM magic \n",
        "    return x, loss\n",
        "\n",
        "# Training Loop\n",
        "for i in range(TR_NO_ITERATIONS):\n",
        "#   if USE_PATTERN_POOL:\n",
        "#     batch = pool.sample(BATCH_SIZE)\n",
        "#     x0 = np.copy(batch.x)\n",
        "#     y0 = batch.y\n",
        "#     # we want half of them new. We remove 1/4 from the top and 1/4 from the\n",
        "#     # bottom.\n",
        "#     q_bs = BATCH_SIZE // 4\n",
        "\n",
        "#     new_idx = np.random.randint(0, x_train.shape[0]-1, size=q_bs)\n",
        "#     x0[:q_bs] = ca.initialize(x_train[new_idx])\n",
        "#     y0[:q_bs] = y_train_pic[new_idx]\n",
        "\n",
        "#     new_idx = np.random.randint(0, x_train.shape[0]-1, size=q_bs)\n",
        "#     new_x, new_y = x_train[new_idx], y_train_pic[new_idx]\n",
        "#     if MUTATE_POOL:\n",
        "#       new_x = tf.reshape(new_x, [q_bs, 28, 28, 1])\n",
        "#       mutate_mask = tf.cast(new_x > 0.1, tf.float32)\n",
        "#       mutated_x = tf.concat([new_x, x0[-q_bs:,:,:,1:] * mutate_mask], -1)\n",
        "\n",
        "#       x0[-q_bs:] = mutated_x\n",
        "#       y0[-q_bs:] = new_y\n",
        "\n",
        "#     else:\n",
        "#       x0[-q_bs:] = ca.initialize(new_x)\n",
        "#       y0[-q_bs:] = new_y\n",
        "\n",
        "#   else:\n",
        "#     b_idx = np.random.randint(0, x_train.shape[0]-1, size=BATCH_SIZE)\n",
        "#     x0 = ca.initialize(x_train[b_idx])\n",
        "#     y0 = y_train_pic[b_idx]\n",
        "\n",
        "    b_idx = np.random.randint(0, x_train.shape[0] - 1, size=BATCH_SIZE)\n",
        "    x0 = ca.initialize(x_train[b_idx])\n",
        "    y0 = y_train_pic[b_idx]\n",
        "\n",
        "    x, loss = train_step(x0, y0)\n",
        "\n",
        "#   if USE_PATTERN_POOL:\n",
        "#     batch.x[:] = x\n",
        "#     batch.y[:] = y0 # This gets reordered, so you need to change it.\n",
        "#     batch.commit()\n",
        "\n",
        "    loss_log[i] = loss.numpy()\n",
        "\n",
        "#   if step_i%100 == 0:\n",
        "#     generate_pool_figures(ca, pool, step_i)\n",
        "    if i % 200 == 0:        \n",
        "        ax.clear()\n",
        "        ax.plot(np.log10(loss_log[:i]), '.', alpha=0.1)\n",
        "        plt.show()\n",
        "\n",
        "#     visualize_batch(ca, x0, x, step_i)\n",
        "#     plot_loss(loss_log)\n",
        "    if i % 10000 == 0:\n",
        "        export_model(ca, 'train_log/%07d'%i)\n",
        "\n",
        "    print('\\r step: %d, log10(loss): %.3f'%(i + 1, np.log10(loss)), end='')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:7 out of the last 7 calls to <function CAModel.call at 0x7eff74097400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-abe5b003695f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#   if USE_PATTERN_POOL:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y0' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9_9yx0mKhdm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}